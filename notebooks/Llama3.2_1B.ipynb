{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOpUiyP819uI"
   },
   "source": [
    "Simply loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fNtyMk71qBe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TransformerLa\n",
    "import torch\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"   # llama 3.1 8B is a scaled up ( in terms of layers) version of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkyMUf152Ai0"
   },
   "source": [
    "Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "af6657ca8f4d4bcd843a0f624df58c38",
      "946a207848c446b68e3947ac70c5a1dd",
      "0c7dd77c90eb4624bac009beeb8bd2a1",
      "081d8f9f6f3849708afaabef096fac31",
      "b4dbe3657fbc404ead61416cceb72877",
      "abe7fb7aa11f476d946cf9b6560c1eb1",
      "7d6fcf6ca2434b46992d6689d5da8fd0",
      "87a00a3218df437e9e814cd0a5b05194",
      "f13b63c800f34813ab0b8ef013c77192",
      "3e074c4459c34389a9ee60f83bbd7f73",
      "99998a6ba4444b4395bc8cbedd2230ed",
      "ce681018e131445b86c00b84688100e4",
      "554666d039ce4995aa2e2b49476aeeec",
      "5a27d31301c740ca9c839af28f76e756",
      "03e9e8087e0d493a9ed781131a8b2fb5",
      "4f133b06c3b14481ab17c04f6392cbce",
      "6bdc1949a7ee4bfebbe5de61689baa11",
      "cd66605d57d44dd9923a069975836799",
      "604b99c4e1314f77aead7100f7b01f60",
      "8dcc7fb6414645eda1769b610762312b",
      "d596fe3c7b7942e6a0ced0861f9e659d",
      "154a092a324048b3bd57d3a55d11c735",
      "b6fb2734d0954a6aac306797c50607c8",
      "56d6072c5a0f416dae4a2aca85b223e2",
      "6fe607e0e734478ebecadd9567ccc892",
      "25d54c21407a4a469812e5b450cfb176",
      "fd1085fc1df44970a2132ea14ee23877",
      "5f53ca196b984efaaa91d6d8c388b45e",
      "61201829f3e94548a1ac1553fae5caac",
      "ff00569deb844aa29c387573b30cfa84",
      "3aed796439e349559870b6f64bce733f",
      "5fb12d935ae6466e9c285f506cbf0106",
      "b4a0da55e1cd447ca81283e9fc27d712"
     ]
    },
    "id": "Q2GSLKHK12XH",
    "outputId": "4006bc76-8563-4e0c-d6e4-11bae89c3afe"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ5SSMru18x1",
    "outputId": "c8a58cb0-8aaa-42d5-e8b4-3535968f0d68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   1883,    645,    527,   2579]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\" Apples are red\"\"\"\n",
    "tokenized_inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK7H1vwz2Nir"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "63ba1c6fa0ce4adf9122fd190d568fb4",
      "869d09fc481744fc8b86045f2ac1cf97",
      "f3dc30c0be114b48a30448d13a7bb6a2",
      "92bf9658f6d74cd096ba9e9d18c95e7d",
      "71bccc2b3ba148898387216175fba563",
      "d0b3dc39da0e42318526608ce22deb35",
      "485ab1fff9ea46df9c6b73b6781baadf",
      "02d174ee7bc24bd5b745e1439de42f19",
      "6a6bd77163674d9da8f68dd35d55cbeb",
      "50df205f945f46d380e68d71ac626c7a",
      "40390877ae374545a380988f0012af4d",
      "bea717058e9f4232b99bdb2d7b3f3e7a",
      "ae78e04dada14d3d91b6117363788a95",
      "5ff8e3aef74b4dc48e758cee96eb6ddf",
      "67398c68764946d4a917ddf0415fa05f",
      "67894490a5fa492fbfc6da7ebf6d76d7",
      "2de3acc813b5412ea8e8bb277c7ed8d1",
      "1230632fc26346c0b2adf23432a951bf",
      "aca04714a3e34af59ddcaffd1027fc00",
      "014c7402c6634f9bb11c14e0364dd86f",
      "6e6997eba93f4c2db0facae02c69d14c",
      "fd13f5602e464ee29a6a4d5a2c5352b2",
      "73ab3ef9b0bc488fa17791111b014dd1",
      "712d16a8a95e4ab3a685a20c5a1021a6",
      "3819fbe6b844428282f5c49bd37009a8",
      "ef58129592154ce3aba33e7b3e733abc",
      "f154605b22274ad18a4e15ee4a342cde",
      "122d2574a4404e6c880ba42f6efacfde",
      "747198c54e944bd98c971341c141a8fe",
      "7562cffef88f464a995b8daa9e5728e4",
      "3d6a628770284c398272f776b1c43775",
      "a2a0948f3e374d87bf6cd6a175207d7f",
      "2dec6c4659a149c0818fbfa2dc799f4a"
     ]
    },
    "id": "3nJr8Ndt2OgC",
    "outputId": "93b4bea6-93ff-41b8-e5b2-893fb7c92b80"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SNHZKGjOZ-o",
    "outputId": "fb92287d-7e05-453e-c4fd-0c39a899f540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.47.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auyJU02cN9nB",
    "outputId": "40cd2772-e011-457e-83df-ef74d7f1b471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([128256, 2048])\n"
     ]
    }
   ],
   "source": [
    "model_weights = model.state_dict()\n",
    "for k, v in model_weights.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-UH-GBTRv_Y"
   },
   "source": [
    "Model Remaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEfES8sKRzB4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06nrwUlRRxfL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEV1XHtNSAie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM5pUkcNSFMR"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, kv_dim, i):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_heads_kv = num_heads / 4\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.q_proj = model_weights[f\"model.layers.{i}.self_attn.q_proj.weight\"] #nn.Linear(embed_dim, num_heads * head_dim)  # Projection for Q\n",
    "        self.k_proj = model_weights[f\"model.layers.{i}.self_attn.k_proj.weight\"].T #nn.Linear(embed_dim, num_heads_kv * head_dim)  # Projection for K\n",
    "        self.v_proj = model_weights[f\"model.layers.{i}.self_attn.v_proj.weight\"].T #nn.Linear(embed_dim, num_heads_kv * head_dim)  # Projection for V\n",
    "        self.o_proj = model_weights[f\"model.layers.{i}.self_attn.o_proj.weight\"] #nn.Linear(embed_dim, num_heads * head_dim)  # Output projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len, embed_dim = x.size()\n",
    "\n",
    "\n",
    "        # Project Q, K, and V\n",
    "        q = torch.matmul(x, self.q_proj)  # (bsz, seq_len, num_heads * head_dim)\n",
    "        k = torch.matmul(x, self.k_proj)  # (bsz, seq_len, num_heads_kv * head_dim)\n",
    "        v = torch.matmul(x, self.v_proj)  # (bsz, seq_len, num_heads_kv * head_dim)\n",
    "\n",
    "        # Transpose to get (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "\n",
    "        # Combine heads and pass through output projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seq_len, embed_dim)\n",
    "        return torch.matmul(attn_output, self.o_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwB52PFDPS-W"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, i):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.gate_proj = model_weights[f\"model.layers.{i}.mlp.gate_proj.weight\"].T #nn.Linear(embed_dim, ff_dim)\n",
    "        self.up_proj = model_weights[f\"model.layers.{i}.mlp.up_proj.weight\"].T #nn.Linear(embed_dim, ff_dim)\n",
    "        self.down_proj = model_weights[f\"model.layers.{i}.mlp.down_proj.weight\"].T #nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Gated Linear Unit (GLU)\n",
    "        gate_x = F.gelu(x @ self.gate_proj)\n",
    "        up_x = x @ self.up_proj\n",
    "        x = gate_x * up_x\n",
    "        x = x @ self.down_proj\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, kv_dim, i):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.self_attn = SelfAttention(embed_dim, num_heads, kv_dim, i)\n",
    "        self.input_layernorm = model_weights[f\"model.layers.{i}.input_layernorm.weight\"]  # nn.LayerNorm(embed_dim)\n",
    "        self.post_attention_layernorm = model_weights[f\"model.layers.{i}.post_attention_layernorm.weight\"] # nn.LayerNorm(embed_dim)\n",
    "        self.mlp = FeedForward(embed_dim, ff_dim, i)\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Self-attention block\n",
    "        residual = x\n",
    "\n",
    "        ## input layer normalization\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        x = self.input_layernorm * x\n",
    "\n",
    "        x = self.self_attn(x) + residual\n",
    "\n",
    "\n",
    "        # Feed-forward block\n",
    "        residual = x\n",
    "\n",
    "        ## input layer normalization\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # this means last dimension layer normalization\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        x = self.input_layernorm * x\n",
    "\n",
    "        x = self.mlp(x) + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, ff_dim, num_heads, kv_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_tokens = model_weights['model.embed_tokens.weight'] # here\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(embed_dim, ff_dim, num_heads, kv_dim, i)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm = model_weights['model.norm.weight'] # nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = model_weights['lm_head.weight'].T # nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Embed tokens\n",
    "\n",
    "        tokenized_output = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        input_ids = tokenized_output[\"input_ids\"]\n",
    "\n",
    "        x = (self.embed_tokens)[input_ids]\n",
    "\n",
    "        # Pass through each layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Normalize output\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(variance + self.eps)  # Layer normalization\n",
    "        x = x * self.norm  # Scale\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        logits = x @ self.lm_head\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMecUxYkRpv4",
    "outputId": "b66acb46-d493-4736-f83e-cc4ca4c5613f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 5, 128256])\n",
      "Token IDs: tensor([[17429, 35178,  8350,  6641,   384]], device='cuda:0')\n",
      "Decoded Tokens: [' nursullafordRED e']\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters based on layer structure\n",
    "vocab_size = 128256\n",
    "embed_dim = 2048\n",
    "num_layers = 16\n",
    "ff_dim = 8192\n",
    "num_heads = 8  # Based on 2048 / 8 = 256 per head\n",
    "kv_dim = 512  # Separate projection sizes for k and v\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(vocab_size, embed_dim, num_layers, ff_dim, num_heads, kv_dim)\n",
    "\n",
    "# Example\n",
    "sentence = \"apples are \"\n",
    "logits = model.forward(sentence)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "\n",
    "token_ids = torch.argmax(logits, dim=-1)  # Shape: [batch_size, seq_len]\n",
    "decoded_tokens = [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
